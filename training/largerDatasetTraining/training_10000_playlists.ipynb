{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNcl0PdnJWy5",
        "outputId": "dcbde9ac-e132-4393-aa0d-a0f1bb6b2297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.8/dist-packages (2.21.0)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.8/dist-packages (from spotipy) (4.4.0)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (2.28.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.26.13)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from redis>=3.5.3->spotipy) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo\n",
        "\n",
        "!pip install spotipy\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP7rBc2GUUlG",
        "outputId": "e68f3c85-f889-4fcd-d2a8-eead8afb5064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.8/dist-packages (0.21.0)\n"
          ]
        }
      ],
      "source": [
        "# load spotify api credentials from .env file\n",
        "\n",
        "# Upload the .env file to the colab, then you'll be able to use the spotify API (sp)\n",
        "\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
        "CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhduHuCfT9KZ",
        "outputId": "4594538f-c0a3-4e20-91be-f272af76b18c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spotipy.client.Spotify at 0x7fa6cd558e80>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Authentication - without user\n",
        "client_credentials_manager = SpotifyClientCredentials(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
        "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)\n",
        "sp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = \"/content/subset\" \n",
        "\n",
        "# read json files of playlists.\n",
        "# returns array of playlists (playlists -> array of track ids)\n",
        "def get_playlists_from_json(fileNames):\n",
        "  playlists = [];\n",
        "\n",
        "  for fileName in fileNames:\n",
        "    f = open(datasetPath + \"/\" + fileName)\n",
        "    resp = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "    for playlist in resp[\"playlists\"]:\n",
        "      track_ids = []\n",
        "      for track in playlist[\"tracks\"]:\n",
        "        track_ids.append(track[\"track_uri\"])\n",
        "      playlists.append(track_ids)\n",
        "  \n",
        "  return playlists"
      ],
      "metadata": {
        "id": "fO6THxYLOMBH",
        "cellView": "code"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playlists = get_playlists_from_json([\"mpd.slice.0-999.json\"])\n",
        "playlists2 = get_playlists_from_json([\"mpd.slice.1000-1999.json\"])\n",
        "playlists3 = get_playlists_from_json([\"mpd.slice.2000-2999.json\"])\n",
        "playlists4 = get_playlists_from_json([\"mpd.slice.3000-3999.json\"])\n",
        "playlists5 = get_playlists_from_json([\"mpd.slice.4000-4999.json\"])\n",
        "playlists6 = get_playlists_from_json([\"mpd.slice.5000-5999.json\"])\n",
        "playlists7 = get_playlists_from_json([\"mpd.slice.6000-6999.json\"])\n",
        "playlists8 = get_playlists_from_json([\"mpd.slice.7000-7999.json\"])\n",
        "playlists9 = get_playlists_from_json([\"mpd.slice.8000-8999.json\"])\n",
        "playlists10 = get_playlists_from_json([\"mpd.slice.9000-9999.json\"])\n",
        "playlists = playlists + playlists2 + playlists3 + playlists4 + playlists5 + playlists6 + playlists7 + playlists8 + playlists9 + playlists10\n",
        "len(playlists)"
      ],
      "metadata": {
        "id": "MbboxXPXPL4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf46b1b-1869-4b86-f829-14bd61161f1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly generate a triplet tuple (anchor, positive, negative)\n",
        "def random_triplet_generator():\n",
        "  random_playlist = random.sample(range(10000), 2)\n",
        "  playlist_len1 = len(playlists[random_playlist[0]])\n",
        "  playlist_len2 = len(playlists[random_playlist[1]])\n",
        "  random_indices = random.sample(range(playlist_len1), 2)\n",
        "  random_index_neg = random.randint(0, len(playlists[random_playlist[1]])-1)\n",
        "  random_song_anchor = random_indices[0]\n",
        "  random_song_positive = random_indices[1]\n",
        "\n",
        "  anchor = playlists[random_playlist[0]][random_song_anchor]\n",
        "  \n",
        "  positive = playlists[random_playlist[0]][random_song_positive]\n",
        "\n",
        "  while True:\n",
        "    if playlists[random_playlist[1]][random_index_neg] not in playlists[random_playlist[0]]:\n",
        "      negative = playlists[random_playlist[1]][random_index_neg]\n",
        "      break\n",
        "    else:\n",
        "      random_index_neg = random.randint(0, len(playlists[random_playlist[1]])-1)\n",
        "  \n",
        "  return anchor, positive, negative"
      ],
      "metadata": {
        "id": "KoL5AJWFTle3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of pairs to randomly generate.\n",
        "NUM_PAIRS = 5000\n",
        "\n",
        "# Get feature vectors for the track id triplets.\n",
        "def data_to_feature_vector_triplets():\n",
        "  pairings = []\n",
        "  for i in range(NUM_PAIRS):\n",
        "    pairings.append(random_triplet_generator())\n",
        "\n",
        "  pairings_as_feature_vectors = []\n",
        "  for each_tuple in pairings:\n",
        "    resp = sp.audio_features(each_tuple)\n",
        "    resp = [list(x.values())[:11] for x in resp]\n",
        "    pairings_as_feature_vectors.append(resp)\n",
        "  return pairings, pairings_as_feature_vectors         "
      ],
      "metadata": {
        "id": "Bwrh1JvzZeFj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairings, feature_vectors = data_to_feature_vector_triplets()"
      ],
      "metadata": {
        "id": "o36MttLGeOD7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save ids & triplets to txt file.\n",
        "\n",
        "f = open(\"triplets.txt\", \"w\")\n",
        "for ids, features in zip(pairings, feature_vectors):\n",
        "  ids_str = \" \".join([id.split(\":\")[-1] for id in ids])\n",
        "  features_str = \" \".join([str(tuple(f)).replace(\" \", \"\") for f in features])\n",
        "  f.write(ids_str + \" \" + features_str + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "G-_DwBCuXwoi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load triplets from triplets.txt file.\n",
        "\n",
        "import ast\n",
        "\n",
        "id_triplet_tuples = []\n",
        "features_triplet_tuples = []\n",
        "\n",
        "with open(\"triplets.txt\") as file:\n",
        "    for line in file:\n",
        "        arr = line.strip().split(\" \")\n",
        "        if len(arr) == 6:\n",
        "          id_triplet_tuples.append((arr[0], arr[1], arr[2]))\n",
        "          features_triplet_tuples.append((ast.literal_eval(arr[3]), ast.literal_eval(arr[4]), ast.literal_eval(arr[5])))\n",
        "\n",
        "print(id_triplet_tuples[:5])\n",
        "print(features_triplet_tuples[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTiZ425VdUyo",
        "outputId": "243dc901-fc07-4dd4-c61c-a739ce7c70e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('55DuTmvSGwNQR4cBgqYuYL', '6V2D8Lls36APk0THDjBDfE', '6KvhbFSMQOCA5PSqPnTtBA'), ('2p07VcUwRZ5sru3mJ0JogS', '6s9ICeczYOfbHHIaSMq9jd', '31bf9SEOppLU6lQ85d8om6'), ('6s9m5J92By7jii22Q2XtY2', '7kB1UXxStzSa78NdiexiIS', '7rWoskZwxQiLsFfRXxFF50'), ('373zqV0VLz9mnrSaY9kaiX', '13qjycCLStZb9sJje6v0MC', '2Fhm0O9VeJdSZgxNiBZTJJ'), ('1mKXFLRA179hdOWQBwUk9e', '0c1gHntWjKD7QShC8s99sq', '0fuQ65fX8W94q6QwTFyqgI')]\n",
            "[((0.584, 0.531, 0, -6.907, 1, 0.375, 0.0115, 0, 0.126, 0.346, 94.907), (0.794, 0.522, 8, -7.829, 1, 0.159, 0.0328, 0, 0.156, 0.567, 86.318), (0.801, 0.839, 1, -3.267, 0, 0.24, 0.0206, 7.08e-05, 0.0946, 0.792, 170.013)), ((0.677, 0.776, 5, -5.933, 1, 0.0386, 0.42, 0.000689, 0.0954, 0.642, 121.834), (0.497, 0.927, 0, -4.52, 1, 0.0371, 0.00286, 1.26e-06, 0.413, 0.607, 96.991), (0.818, 0.653, 1, -8.396, 1, 0.204, 0.0335, 5.46e-06, 0.22, 0.533, 99.931)), ((0.68, 0.888, 9, -5.308, 1, 0.055, 0.0527, 2.82e-06, 0.0575, 0.484, 90.076), (0.453, 0.587, 4, -7.584, 1, 0.0406, 0.475, 0, 0.109, 0.625, 145.334), (0.393, 0.983, 1, -3.168, 1, 0.0654, 2.05e-05, 0.765, 0.57, 0.823, 144.016)), ((0.481, 0.826, 1, -6.974, 0, 0.0851, 0.182, 0.00352, 0.324, 0.819, 118.679), (0.402, 0.867, 4, -3.969, 0, 0.0843, 2.11e-05, 0.000185, 0.0705, 0.38, 104.092), (0.558, 0.967, 3, -4.051, 1, 0.127, 0.000163, 1.69e-05, 0.227, 0.567, 139.975)), ((0.778, 0.547, 2, -7.273, 1, 0.0489, 0.346, 0.000302, 0.132, 0.441, 95.002), (0.39, 0.649, 2, -5.163, 1, 0.0345, 0.172, 0, 0.216, 0.144, 121.16), (0.576, 0.694, 2, -8.058, 1, 0.0359, 0.0139, 0.907, 0.0777, 0.279, 95.02))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class TripletLossLayer(Layer):\n",
        "  def __init__(self, alpha, **kwargs):\n",
        "    self.alpha=alpha\n",
        "    super(TripletLossLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "        'aplha':self.alpha\n",
        "    })\n",
        "    return config\n",
        "  \n",
        "  def triplet_loss(self, inputs):\n",
        "    a, p, n, = inputs\n",
        "    p_dist = K.sum(K.square(a-p), axis=-1)\n",
        "    n_dist = K.sum(K.square(a-n), axis=1)\n",
        "    return K.sum(K.maximum(p_dist-n_dist+self.alpha,0),axis=0)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    loss = self.triplet_loss(inputs)\n",
        "    self.add_loss(loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "oSkLO8r0gyXA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Input for anchor, positive, and negative images\n",
        "in_a = Input(shape=(11, 1), name=\"song_a\")\n",
        "in_p = Input(shape=(11, 1), name=\"song_p\")\n",
        "in_n = Input(shape=(11, 1), name=\"song_n\")\n",
        "\n",
        "# create the base model\n",
        "base = layers.Dense(11)(in_a)\n",
        "flatten = layers.Flatten()(base)\n",
        "dense = layers.Dense(11, activation=\"relu\")(flatten) \n",
        "dense = layers.BatchNormalization()(dense)\n",
        "output = layers.Dense(11)(dense)\n",
        "\n",
        "embedding = Model(in_a, output, name=\"Embedding\")\n",
        "\n",
        "# Custom vector representation of each song\n",
        "emb_a, emb_p, emb_n = embedding(in_a), embedding(in_p), embedding(in_n)\n",
        "\n",
        "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
        "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
        "\n",
        "# Model that can be trained with anchor, positive, and negative feature vectors\n",
        "model = Model([in_a, in_p, in_n], triplet_loss_layer, name=\"model_2\") \n",
        "model.compile(loss=None, optimizer='adam')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CJTDDE6zg33V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6240b65a-50a5-4d09-b29b-41cfced734e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " song_a (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " song_p (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " song_n (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " Embedding (Functional)         (None, 11)           1540        ['song_a[0][0]',                 \n",
            "                                                                  'song_p[0][0]',                 \n",
            "                                                                  'song_n[0][0]']                 \n",
            "                                                                                                  \n",
            " triplet_loss_layer (TripletLos  ()                  0           ['Embedding[0][0]',              \n",
            " sLayer)                                                          'Embedding[1][0]',              \n",
            "                                                                  'Embedding[2][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,540\n",
            "Trainable params: 1,518\n",
            "Non-trainable params: 22\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "# format the triplets to be compatible with the .fit()\n",
        "input_a = convert_to_tensor([i[0] for i in features_triplet_tuples])\n",
        "input_p = convert_to_tensor([i[1] for i in features_triplet_tuples])\n",
        "input_n = convert_to_tensor([i[2] for i in features_triplet_tuples])"
      ],
      "metadata": {
        "id": "GvwUidm0U-4s"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't run fit if you already have the weights.hdf5 file.\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Training the Model\n",
        "EPOCHS = 100 # Max number of epochs\n",
        "\n",
        "model.fit([input_a, input_p, input_n], \n",
        "    epochs = EPOCHS, \n",
        "    callbacks=[ModelCheckpoint(filepath='weights.hdf5',\n",
        "                                monitor = 'loss',\n",
        "                                save_best_only = True,\n",
        "                                mode = 'auto',\n",
        "                                save_weights_only = True,\n",
        "                                verbose = 1),\n",
        "                EarlyStopping(monitor='loss',\n",
        "                              mode='auto',\n",
        "                              patience=10,\n",
        "                              verbose=True)])"
      ],
      "metadata": {
        "id": "77vmbmkng7qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7fae53-e9cd-4a72-ceb7-988a450d0a0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 11.4248\n",
            "Epoch 1: loss improved from inf to 11.29639, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 11.2964\n",
            "Epoch 2/100\n",
            "151/157 [===========================>..] - ETA: 0s - loss: 6.5058\n",
            "Epoch 2: loss improved from 11.29639 to 6.47467, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.4747\n",
            "Epoch 3/100\n",
            "151/157 [===========================>..] - ETA: 0s - loss: 5.7346\n",
            "Epoch 3: loss improved from 6.47467 to 5.73445, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.7344\n",
            "Epoch 4/100\n",
            "155/157 [============================>.] - ETA: 0s - loss: 5.4298\n",
            "Epoch 4: loss improved from 5.73445 to 5.39839, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 5.3984\n",
            "Epoch 5/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 5.4618\n",
            "Epoch 5: loss did not improve from 5.39839\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 5.4360\n",
            "Epoch 6/100\n",
            "149/157 [===========================>..] - ETA: 0s - loss: 5.6379\n",
            "Epoch 6: loss did not improve from 5.39839\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.5728\n",
            "Epoch 7/100\n",
            "147/157 [===========================>..] - ETA: 0s - loss: 5.2685\n",
            "Epoch 7: loss improved from 5.39839 to 5.20849, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.2085\n",
            "Epoch 8/100\n",
            "148/157 [===========================>..] - ETA: 0s - loss: 5.1688\n",
            "Epoch 8: loss improved from 5.20849 to 5.15758, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1576\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 5.0926\n",
            "Epoch 9: loss improved from 5.15758 to 5.09263, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0926\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 5.1264\n",
            "Epoch 10: loss did not improve from 5.09263\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1264\n",
            "Epoch 11/100\n",
            "155/157 [============================>.] - ETA: 0s - loss: 5.0812\n",
            "Epoch 11: loss improved from 5.09263 to 5.06387, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 5.0639\n",
            "Epoch 12/100\n",
            "152/157 [============================>.] - ETA: 0s - loss: 5.1563\n",
            "Epoch 12: loss did not improve from 5.06387\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 5.1312\n",
            "Epoch 13/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 5.0657\n",
            "Epoch 13: loss improved from 5.06387 to 5.04502, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0450\n",
            "Epoch 14/100\n",
            "152/157 [============================>.] - ETA: 0s - loss: 5.0499\n",
            "Epoch 14: loss improved from 5.04502 to 5.01297, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0130\n",
            "Epoch 15/100\n",
            "154/157 [============================>.] - ETA: 0s - loss: 4.9852\n",
            "Epoch 15: loss improved from 5.01297 to 4.96755, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9676\n",
            "Epoch 16/100\n",
            "155/157 [============================>.] - ETA: 0s - loss: 5.0077\n",
            "Epoch 16: loss did not improve from 4.96755\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9856\n",
            "Epoch 17/100\n",
            "148/157 [===========================>..] - ETA: 0s - loss: 5.1151\n",
            "Epoch 17: loss did not improve from 4.96755\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0539\n",
            "Epoch 18/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 5.0434\n",
            "Epoch 18: loss did not improve from 4.96755\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 5.0172\n",
            "Epoch 19/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 4.9810\n",
            "Epoch 19: loss improved from 4.96755 to 4.96750, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 4.9675\n",
            "Epoch 20/100\n",
            "146/157 [==========================>...] - ETA: 0s - loss: 4.9789\n",
            "Epoch 20: loss improved from 4.96750 to 4.91058, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 4.9106\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 5.0548\n",
            "Epoch 21: loss did not improve from 4.91058\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0548\n",
            "Epoch 22/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 4.9654\n",
            "Epoch 22: loss did not improve from 4.91058\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9238\n",
            "Epoch 23/100\n",
            "149/157 [===========================>..] - ETA: 0s - loss: 4.8739\n",
            "Epoch 23: loss improved from 4.91058 to 4.86850, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8685\n",
            "Epoch 24/100\n",
            "151/157 [===========================>..] - ETA: 0s - loss: 4.8612\n",
            "Epoch 24: loss improved from 4.86850 to 4.83559, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8356\n",
            "Epoch 25/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 4.9367\n",
            "Epoch 25: loss did not improve from 4.83559\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9177\n",
            "Epoch 26/100\n",
            "155/157 [============================>.] - ETA: 0s - loss: 4.8417\n",
            "Epoch 26: loss did not improve from 4.83559\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8390\n",
            "Epoch 27/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 4.8780\n",
            "Epoch 27: loss did not improve from 4.83559\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8598\n",
            "Epoch 28/100\n",
            "151/157 [===========================>..] - ETA: 0s - loss: 4.8548\n",
            "Epoch 28: loss improved from 4.83559 to 4.82256, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8226\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.8320\n",
            "Epoch 29: loss did not improve from 4.82256\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8320\n",
            "Epoch 30/100\n",
            "149/157 [===========================>..] - ETA: 0s - loss: 4.8504\n",
            "Epoch 30: loss did not improve from 4.82256\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8527\n",
            "Epoch 31/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 4.8056\n",
            "Epoch 31: loss improved from 4.82256 to 4.79129, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7913\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.8625\n",
            "Epoch 32: loss did not improve from 4.79129\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8625\n",
            "Epoch 33/100\n",
            "148/157 [===========================>..] - ETA: 0s - loss: 4.8585\n",
            "Epoch 33: loss did not improve from 4.79129\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8038\n",
            "Epoch 34/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 4.8970\n",
            "Epoch 34: loss did not improve from 4.79129\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8414\n",
            "Epoch 35/100\n",
            "148/157 [===========================>..] - ETA: 0s - loss: 4.7826\n",
            "Epoch 35: loss improved from 4.79129 to 4.77925, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7793\n",
            "Epoch 36/100\n",
            "154/157 [============================>.] - ETA: 0s - loss: 4.8174\n",
            "Epoch 36: loss did not improve from 4.77925\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7847\n",
            "Epoch 37/100\n",
            "147/157 [===========================>..] - ETA: 0s - loss: 4.8149\n",
            "Epoch 37: loss improved from 4.77925 to 4.76892, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7689\n",
            "Epoch 38/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 4.8176\n",
            "Epoch 38: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7939\n",
            "Epoch 39/100\n",
            "156/157 [============================>.] - ETA: 0s - loss: 4.8215\n",
            "Epoch 39: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7964\n",
            "Epoch 40/100\n",
            "147/157 [===========================>..] - ETA: 0s - loss: 4.8292\n",
            "Epoch 40: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7968\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.7832\n",
            "Epoch 41: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7832\n",
            "Epoch 42/100\n",
            "150/157 [===========================>..] - ETA: 0s - loss: 4.7817\n",
            "Epoch 42: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7954\n",
            "Epoch 43/100\n",
            "149/157 [===========================>..] - ETA: 0s - loss: 4.8123\n",
            "Epoch 43: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 4.7948\n",
            "Epoch 44/100\n",
            "156/157 [============================>.] - ETA: 0s - loss: 4.8146\n",
            "Epoch 44: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 4.8090\n",
            "Epoch 45/100\n",
            "148/157 [===========================>..] - ETA: 0s - loss: 4.7783\n",
            "Epoch 45: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7790\n",
            "Epoch 46/100\n",
            "147/157 [===========================>..] - ETA: 0s - loss: 4.8440\n",
            "Epoch 46: loss did not improve from 4.76892\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8165\n",
            "Epoch 47/100\n",
            "152/157 [============================>.] - ETA: 0s - loss: 4.7720\n",
            "Epoch 47: loss improved from 4.76892 to 4.74638, saving model to weights.hdf5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7464\n",
            "Epoch 48/100\n",
            "153/157 [============================>.] - ETA: 0s - loss: 4.7743\n",
            "Epoch 48: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7694\n",
            "Epoch 49/100\n",
            "155/157 [============================>.] - ETA: 0s - loss: 4.8528\n",
            "Epoch 49: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8324\n",
            "Epoch 50/100\n",
            "154/157 [============================>.] - ETA: 0s - loss: 4.9696\n",
            "Epoch 50: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9383\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.7878\n",
            "Epoch 51: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7878\n",
            "Epoch 52/100\n",
            "152/157 [============================>.] - ETA: 0s - loss: 4.8222\n",
            "Epoch 52: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8149\n",
            "Epoch 53/100\n",
            "149/157 [===========================>..] - ETA: 0s - loss: 4.8808\n",
            "Epoch 53: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8533\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.8363\n",
            "Epoch 54: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8363\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.7690\n",
            "Epoch 55: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7690\n",
            "Epoch 56/100\n",
            "156/157 [============================>.] - ETA: 0s - loss: 4.7794\n",
            "Epoch 56: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7490\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 4.7735\n",
            "Epoch 57: loss did not improve from 4.74638\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7735\n",
            "Epoch 57: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6c9c00d90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best weights file created by .fit()\n",
        "model = Model([in_a, in_p, in_n], triplet_loss_layer) \n",
        "model.load_weights(\"./weights.hdf5\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64NeJlFkcaDR",
        "outputId": "0450419c-8d2c-4b94-db24-a27f1c16810b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " song_a (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " song_p (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " song_n (InputLayer)            [(None, 11, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " Embedding (Functional)         (None, 11)           1540        ['song_a[0][0]',                 \n",
            "                                                                  'song_p[0][0]',                 \n",
            "                                                                  'song_n[0][0]']                 \n",
            "                                                                                                  \n",
            " triplet_loss_layer (TripletLos  ()                  0           ['Embedding[0][0]',              \n",
            " sLayer)                                                          'Embedding[1][0]',              \n",
            "                                                                  'Embedding[2][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,540\n",
            "Trainable params: 1,518\n",
            "Non-trainable params: 22\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bypass the triplet input layers (allows us to just input a single song)\n",
        "base_model = model.get_layer(\"Embedding\")\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "MkcBDplyc4KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d0a572-a744-4fa0-9b5f-6fe0a3710c3c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Embedding\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " song_a (InputLayer)         [(None, 11, 1)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11, 11)            22        \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 121)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 11)                1342      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 11)               44        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 11)                132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,540\n",
            "Trainable params: 1,518\n",
            "Non-trainable params: 22\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "existing = set()\n",
        "spotify_features_dataset = []\n",
        "\n",
        "# read tracks & feature vectors from output.csv\n",
        "with open('/content/output.csv', 'r') as file:\n",
        "    next(file) # don't read in header\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "      if row[12] not in existing:\n",
        "        existing.add(row[12])\n",
        "        spotify_features_dataset.append((row[12], tuple([float(i) for i in row[:11]])))\n",
        "\n",
        "print(spotify_features_dataset[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRY2TJvp1lsx",
        "outputId": "278cb3cb-e7b7-432d-9e57-d1eb1e8d034b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('5IbCV9Icebx8rR6wAp5hhP', (0.451, 0.258, 2.0, -15.947, 1.0, 0.0681, 0.763, 0.0, 0.136, 0.646, 78.62)), ('6rKVAvjHcxAzZ1BHtwh5yC', (0.588, 0.189, 0.0, -17.737, 1.0, 0.0451, 0.756, 0.0, 0.169, 0.904, 140.467)), ('6Jlkb1Wh08RYHstWScsTvg', (0.281, 0.0652, 6.0, -22.218, 1.0, 0.0388, 0.959, 9.75e-06, 0.102, 0.316, 77.442)), ('0XhC8bfStML9ygBmfOt1JJ', (0.746, 0.3, 8.0, -16.037, 1.0, 0.164, 0.682, 0.0, 0.39, 0.842, 130.248)), ('0ABxAcsRWlqckkyONsfP67', (0.493, 0.235, 7.0, -14.847, 1.0, 0.14, 0.732, 0.0, 0.126, 0.455, 81.576))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings (transformed feature vectors) for every song in output.csv\n",
        "lookup_table = {}\n",
        "\n",
        "track_ids = [tup[0] for tup in spotify_features_dataset]\n",
        "track_vectors = [tup[1] for tup in spotify_features_dataset]\n",
        "\n",
        "count = 0\n",
        "while count < len(track_vectors):\n",
        "  result = base_model.predict(track_vectors[count:count+1800])\n",
        "  for (id, vec) in zip(track_ids[count: count+1800], result):\n",
        "    lookup_table[id] = tuple(vec)\n",
        "  count += 1800"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y01hPt_03aWM",
        "outputId": "899003d4-efcf-4d0b-88f5-289a97d4c63b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 3ms/step\n",
            "57/57 [==============================] - 0s 3ms/step\n",
            "57/57 [==============================] - 0s 3ms/step\n",
            "57/57 [==============================] - 0s 3ms/step\n",
            "57/57 [==============================] - 0s 3ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "9/9 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save transformed track vectors to file\n",
        "\n",
        "f = open(\"transformed.txt\", \"w\")\n",
        "for key, value in lookup_table.items():\n",
        "  f.write(key.replace(\" \", \"\") + \" \" + str(value).replace(\" \", \"\") + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "weRqiMRNCt-W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load transformed track vectors from \"transformed.txt\" file\n",
        "# lookup_table = {}\n",
        "\n",
        "# with open('/content/transformed.txt', 'r') as file:\n",
        "#   for line in file:\n",
        "#     arr = line.split(\" \")\n",
        "#     if len(arr) == 2:\n",
        "#       lookup_table[arr[0]] = ast.literal_eval(arr[1])"
      ],
      "metadata": {
        "id": "8ckizuTnDbF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}